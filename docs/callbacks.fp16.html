---

title: callback.fp16
keywords: 
sidebar: home_sidebar

summary: "Training in mixed precision implementation"
---

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Mixed-precision-training">Mixed precision training<a class="anchor-link" href="#Mixed-precision-training">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This module allows the forward and backward passes of your neural net to be done in fp16 (also known as <em>half precision</em>). This is particularly important if you have an NVIDIA GPU with <a href="https://www.nvidia.com/en-us/data-center/tensorcore/">tensor cores</a>, since it can speed up your training by 200% or more.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Overview">Overview<a class="anchor-link" href="#Overview">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To train your model in mixed precision you just have to call <a href="/train.html#to_fp16"><code>Learner.to_fp16</code></a>, which converts the model and modifies the existing <a href="/basic_train.html#Learner"><code>Learner</code></a> to add <a href="/callbacks.fp16.html#MixedPrecision"><code>MixedPrecision</code></a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=to_fp16></a><code>to_fp16</code></h4>
<blockquote><p><code>to_fp16</code>(<code>learn</code>:<a href="/basic_train.html#Learner"><code>Learner</code></a>, <code>loss_scale</code>:<code>float</code>=<code>512.0</code>, <code>flat_master</code>:<code>bool</code>=<code>False</code>) -&gt; <a href="/basic_train.html#Learner"><code>Learner</code></a></p>
</blockquote>
<p>Transform <code>learn</code> in FP16 precision. <a href="https://github.com/fastai/fastai/blob/master/fastai/train.py#L26">[source]</a></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For example:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">ConvLearner</span><span class="p">(</span><span class="n">get_mnist</span><span class="p">(),</span> <span class="n">tvm</span><span class="o">.</span><span class="n">resnet18</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span><span class="o">.</span><span class="n">to_fp16</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




 
 
<div id="f175c992-880d-4565-ab62-cc8150f63ecd"></div>
<div class="output_subarea output_widget_view ">
<script type="text/javascript">
var element = $('#f175c992-880d-4565-ab62-cc8150f63ecd');
</script>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "5ab4931b75eb4781944bf690934bdd22", "version_major": 2, "version_minor": 0}
</script>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Total time: 00:10
epoch  train loss  valid loss  accuracy
0      0.113659    0.068874    0.978901  (00:10)

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Details about mixed precision training are available in <a href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html">NVIDIA's documentation</a>. We will just summarize the basics here.</p>
<p>The only parameter you may want to tweak is <code>loss_scale</code>. This is used to scale the loss up, so that it doesn't underflow fp16, leading to loss of accuracy (this is reversed for the final gradient calculation after converting back to fp32). Generally the default <code>512</code> works well, however. You can also enable or disable the flattening of the master parameter tensor with <code>flat_master=True</code>, however in our testing the different is negligible.</p>
<p>Internally, the callback ensures that all model parameters (except batchnorm layers, which require fp32) are converted to fp16, and an fp32 copy is also saved. The fp32 copy (the <code>master</code> parameters) is what is used for actually updating with the optimizer; the fp16 parameters are used for calculating gradients. This helps avoid underflow with small learning rates.</p>
<p>All of this is implemented by the following Callback.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2><a id=MixedPrecision></a><code>class</code> <code>MixedPrecision</code></h2>
<blockquote><p><code>MixedPrecision</code>(<code>learn</code>:<a href="/basic_train.html#Learner"><code>Learner</code></a>, <code>loss_scale</code>:<code>float</code>=<code>512.0</code>, <code>flat_master</code>:<code>bool</code>=<code>False</code>) :: <a href="/callback.html#Callback"><code>Callback</code></a></p>
</blockquote>
<p>Callback that handles mixed-precision training. <a href="https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L56">[source]</a></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You don't have to call the following functions yourself - they're called by the callback framework automatically. They're just documented here so you can see exactly what the callback is doing.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_backward_begin></a><code>on_backward_begin</code></h4>
<blockquote><p><code>on_backward_begin</code>(<code>last_loss</code>:<code>Rank0Tensor</code>, <code>kwargs</code>:<code>Any</code>) -&gt; <code>Rank0Tensor</code></p>
</blockquote>
<p>Scale gradients up by <code>loss_scale</code> to prevent underflow. <a href="https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L90">[source]</a></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_backward_end></a><code>on_backward_end</code></h4>
<blockquote><p><code>on_backward_end</code>(<code>kwargs</code>:<code>Any</code>)</p>
</blockquote>
<p>Convert the gradients back to FP32 and divide them by the scale. <a href="https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L95">[source]</a></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_loss_begin></a><code>on_loss_begin</code></h4>
<blockquote><p><code>on_loss_begin</code>(<code>last_output</code>:<code>Tensor</code>, <code>kwargs</code>:<code>Any</code>) -&gt; <code>Tensor</code></p>
</blockquote>
<p>Convert half precision output to FP32 to avoid reduction overflow. <a href="https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L86">[source]</a></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_step_end></a><code>on_step_end</code></h4>
<blockquote><p><code>on_step_end</code>(<code>kwargs</code>:<code>Any</code>)</p>
</blockquote>
<p>Update the params from master to model and zero grad. <a href="https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L101">[source]</a></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_train_begin></a><code>on_train_begin</code></h4>
<blockquote><p><code>on_train_begin</code>(<code>kwargs</code>:<code>Any</code>)</p>
</blockquote>
<p>Ensure everything is in half precision mode. <a href="https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L63">[source]</a></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_train_end></a><code>on_train_end</code></h4>
<blockquote><p><code>on_train_end</code>(<code>kwargs</code>:<code>Any</code>)</p>
</blockquote>
<p>Remove half precision transforms added at <code>on_train_begin</code>. <a href="https://github.com/fastai/fastai/blob/master/fastai/callbacks/fp16.py#L80">[source]</a></p>

</div>

</div>

</div>
</div>

</div>
</div>
 

