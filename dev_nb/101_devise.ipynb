{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.docs import *\n",
    "from fastai.text import *\n",
    "torch.backends.cudnn.benchmark=True\n",
    "import json\n",
    "\n",
    "import fastText as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeVise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, we will implement the [DeVise paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf). What makes this paper specially interesting is that it combines image classification and text embeddings. The technique presented by the authors leverages word embeddings to assign several possible tags to each image. By doing this, the model fares considerably well (achieving up to 18% hit rates) in never seen before categories (zero-shot learning). But how can the model classify objects it has never seen before? That is the power of word embeddings.\n",
    "\n",
    "Basically, the model will use the 'closeness' of several words it knows through the embeddings to classify a new image. Perhaps this is easiest explained through a human example. When we are teaching a toddler what a motorcycle is, for example, we might say \"Well, it is a bicycle but it goes faster\". That is we relate it to what *he/she already knows*. In the same way, if the model sees a trout, it might say \"Well, I know it is very similar to a trench and I know what a trench is so I will say it is either a trench or something very similar, like a sea bass or a trout\". In 2D these relationships would look like this:\n",
    "\n",
    "![clusters](imgs/clusters.png)\n",
    "Frome et al., 2013\n",
    "\n",
    "Please consider that while you may say \"Obviously, a goldfish has to do more with a shark than with an iguana because they are both aquatic\" you are comparing these across one dimension, namely natural habitat, while if you compared them by size the results would be different. These infinite dimensions across which you can compare two words are resumed into a finite number of categories which is what we call embeddings. In this image, we are arbitrarily choosing one dimension to make the point since it is intuitive to us human beings.\n",
    "\n",
    "To create this network the authors combined a computer vision architecture with the embeddings data to create a hybrid model that we can see in the following picture:\n",
    "\n",
    "![devise_arch](imgs/devise_arch.png)\n",
    "Frome et al., 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/imagenet')\n",
    "TMP_PATH = Path('../data/imagenet/tmp')\n",
    "TRANS_PATH = Path('../data/translate/')\n",
    "PATH_TRN = PATH/'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to load our word vectors. We'll see that each word has a normalized number between [-1, 1] for each of the 300 embeddings. This is effectively a 300 dimension representation of the meaning of each word. As an example, let's see the embedding for 'king'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-3ab9b4c448c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mft_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRANS_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'wiki.en.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastText/FastText.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;34m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastText/FastText.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ft_vecs = ft.load_model(str((TRANS_PATH/'wiki.en.bin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vecs.get_word_vector('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how correlated two words are in how close these numbers are for each embedding. For example we would stipulate that 'jeremy' and 'Jeremy' are more related than 'banana' and 'Jeremy'. Let's see if our embeddings think alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.60866078],\n",
       "       [0.60866078, 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(ft_vecs.get_word_vector('jeremy'), ft_vecs.get_word_vector('Jeremy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.14482342],\n",
       "       [0.14482342, 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(ft_vecs.get_word_vector('banana'), ft_vecs.get_word_vector('Jeremy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map imagenet classes to word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get all the words in the dictionary and sort them by their frequency (how often do they appear in the aforementioned datasets). We will then count how many words do we have in our dictionary as another step in the 'discovery phase' of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_words = ft_vecs.get_words(include_freq=True)\n",
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}\n",
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the names of our 1000 imagenet classes so that we can assign each class in our imagenet dataset to a 300-long embedding (for that we need the actual word-id for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_FN = 'imagenet_class_index.json'\n",
    "download_url(f'http://files.fast.ai/models/{CLASSES_FN}', TMP_PATH/CLASSES_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also download all the nouns in English from WORDNET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_FN = 'classids.txt'\n",
    "download_url(f'http://files.fast.ai/data/{WORDS_FN}', PATH/WORDS_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will build a dictionary that maps our classes to the word-id for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = json.load((TMP_PATH/CLASSES_FN).open())\n",
    "classids_1k = dict(class_dict.values())\n",
    "nclass = len(class_dict); nclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that our class-id assignments are made correctly. Here we can see our two worlds:\n",
    "\n",
    "1. Imagenet and its class to id mapping\n",
    "2. WORDNET and its class to id mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n00001740 entity\\n',\n",
       " 'n00001930 physical_entity\\n',\n",
       " 'n00002137 abstraction\\n',\n",
       " 'n00002452 thing\\n',\n",
       " 'n00002684 object\\n']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classid_lines = (PATH/WORDS_FN).open().readlines()\n",
    "classid_lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the nouns in the English language and the Imagenet class ids, we need to connect each of these with the words in fastText. We will do this by creating a dictionary of synset to word vectors for both our WORDNET and Imagenet lists that __will only keep the words that are present in both datasets__ (i.e. both WORDNET and fastText for *syn_wv* and both Imagenet and fastText for *syn_wv_1k*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classids = dict(l.strip().split() for l in classid_lines)\n",
    "len(classids),len(classids_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_vec_d = {w.lower(): ft_vecs.get_word_vector(w) for w in ft_words[-1000000:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fe7ccf6ebc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m syn_wv = [(k, lc_vec_d[v.lower()]) for k,v in classids.items()\n\u001b[0m\u001b[1;32m      2\u001b[0m           if v.lower() in lc_vec_d]\n\u001b[1;32m      3\u001b[0m syn_wv_1k = [(k, lc_vec_d[v.lower()]) for k,v in classids_1k.items()\n\u001b[1;32m      4\u001b[0m           if v.lower() in lc_vec_d]\n\u001b[1;32m      5\u001b[0m \u001b[0msyn2wv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn_wv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classids' is not defined"
     ]
    }
   ],
   "source": [
    "syn_wv = [(k, lc_vec_d[v.lower()]) for k,v in classids.items()\n",
    "          if v.lower() in lc_vec_d]\n",
    "syn_wv_1k = [(k, lc_vec_d[v.lower()]) for k,v in classids_1k.items()\n",
    "          if v.lower() in lc_vec_d]\n",
    "syn2wv = dict(syn_wv)\n",
    "len(syn2wv), len(syn_wv_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'syn2wv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-46ea570c57a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn2wv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTMP_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'syn2wv.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn_wv_1k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTMP_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'syn_wv_1k.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'syn2wv' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(syn2wv, (TMP_PATH/'syn2wv.pkl').open('wb'))\n",
    "pickle.dump(syn_wv_1k, (TMP_PATH/'syn_wv_1k.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn2wv = pickle.load((TMP_PATH/'syn2wv.pkl').open('rb'))\n",
    "syn_wv_1k = pickle.load((TMP_PATH/'syn_wv_1k.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is building the data we are going to train our model on. For that we are only including images with ids that are English nouns. Our _x_ variables will be our images (which we are saving in a PosixPath format) and our *y* variables will be our vectors (300 floats, one for each embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "img_vecs = []\n",
    "images_val = []\n",
    "img_vecs_val = []\n",
    "\n",
    "for d in (PATH/'train').iterdir():\n",
    "    if d.name not in syn2wv: continue\n",
    "    vec = syn2wv[d.name]\n",
    "    for f in d.iterdir():\n",
    "        images.append(str(f.relative_to(PATH)))\n",
    "        img_vecs.append(vec)\n",
    "\n",
    "n_val=0\n",
    "for d in (PATH/'valid').iterdir():\n",
    "    if d.name not in syn2wv: continue\n",
    "    vec = syn2wv[d.name]\n",
    "    for f in d.iterdir():\n",
    "        images_val.append(str(f.relative_to(PATH)))\n",
    "        img_vecs_val.append(vec)\n",
    "        n_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28700"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(739526, 300)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vecs = np.stack(img_vecs)\n",
    "img_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(images, (TMP_PATH/'images.pkl').open('wb'))\n",
    "pickle.dump(img_vecs, (TMP_PATH/'img_vecs.pkl').open('wb'))\n",
    "pickle.dump(images_val, (TMP_PATH/'images_val.pkl').open('wb'))\n",
    "pickle.dump(img_vecs_val, (TMP_PATH/'img_vecs)val.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pickle.load((TMP_PATH/'images.pkl').open('rb'))\n",
    "img_vecs = pickle.load((TMP_PATH/'img_vecs.pkl').open('rb'))\n",
    "images_val = pickle.load((TMP_PATH/'images_val.pkl').open('rb'))\n",
    "img_vecs_val = pickle.load((TMP_PATH/'img_vecs_val.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our dataset and create our DataBunch object. Note that we will need to tell our model how many classes we have. We will specify this manually since our ImageDataset class does not support it natively (this argument will then be passed to our model). We will resize our pictures to a 224x224 size and normalize them. Finally we will check that our data looks as we would like it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = (PATH/\"\").absolute()\n",
    "images = [folder_path/image for image in images]\n",
    "images_val = [folder_path/image_val for image_val in images_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739526"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_val[0]\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageDataset(images, img_vecs)\n",
    "valid_ds = ImageDataset(images_val, img_vecs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.classes = range(300)\n",
    "valid_ds.classes = range(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = ([flip_lr()], [crop_pad(size=224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, path=PATH, device=torch.device('cuda'), ds_tfms = get_transforms(), tfms=imagenet_norm, size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 224, 224]), torch.Size([128, 300]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(data.valid_dl))\n",
    "x.size(),y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train our model. Our model will try to predict the value of each embedding for each of our images. To accomplish this we will add a fully connected layer at the end of our resnet50 architecture (with 300 output neurons) and precompute the activations of the backbone model so as to save training time. We will also initialize the weights of the backbone model with the weights of the pretrained model. Given that the pretrained model and ours are both training in the same dataset we will not need to do any finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to start by precomputing our activations for the convolutional backbone and we will then train our head from these activations (so our model will not have to calculate them again for each epoch). Since we already have a pretrained backbone, we will just need to run one forward pass to compute the final activations; that is we will not need any optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz, threading\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner(data, tvm.resnet50, ps=[0.2,0.2], pretrained=True, callback_fns=BnFreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = learn.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(body.children())\n",
    "layers += [AdaptiveConcatPool2d(), Flatten()]   \n",
    "body = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveConcatPool2d(\n",
       "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "  )\n",
       "  (9): Lambda()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = num_features(body)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt_fn = partial(AdamW, betas=(0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_loss(inp,targ): return 1 - F.cosine_similarity(inp,targ).mean()\n",
    "learn.loss_fn = cos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(path, model_name, tmp_path, nf, force=False): \n",
    "    tmpl = f'_{model_name}.bc'\n",
    "    names = [os.path.join(path/'tmp', p+tmpl) for p in ('x_act', 'x_act_val')]\n",
    "    if os.path.exists(names[0]) and not force:\n",
    "        activations = [bcolz.open(p) for p in names]\n",
    "    else:\n",
    "        activations = [create_empty_bcolz(nf,p) for p in names]\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_bcolz(n, name):\n",
    "    return bcolz.carray(np.zeros((0,n), np.float32), chunklen=1, mode='w', rootdir=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_to_bcolz(m, gen, arr, workers=4):\n",
    "    arr.trim(len(arr))\n",
    "    lock=threading.Lock()\n",
    "    m.eval()\n",
    "    for x,*_ in tqdm(gen):\n",
    "        y = to_np(m(x.data))\n",
    "        with lock:\n",
    "            arr.append(y)\n",
    "    arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fc1(data, model, path, model_name, tmp_path, nf):\n",
    "    activations = get_activations(path, model_name, tmp_path, nf)\n",
    "    act=activations[0]\n",
    "    val_act=activations[1]\n",
    "    m=model\n",
    "    if len(act)!=len(data.train_ds):\n",
    "        act = predict_to_bcolz(m, data.train_dl, act)\n",
    "    if len(val_act)!=len(data.valid_ds):\n",
    "        val_act = predict_to_bcolz(m, data.valid_dl, val_act)\n",
    "    \n",
    "    fc_data = FCDataset(act, img_vecs)\n",
    "    fc_data_val = FCDataset(val_act, img_vecs_val)\n",
    "    \n",
    "    fc_data.classes = data.classes\n",
    "    fc_data_val.classes = data.classes\n",
    "    \n",
    "    fc_db = DataBunch.create(fc_data, fc_data_val, path=PATH, device=torch.device('cuda'), bs=32)\n",
    "    return fc_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_features(m:Model)->int:\n",
    "    \"Return the number of output features for a `model`.\"\n",
    "    for l in reversed(flatten_model(m)):\n",
    "        if hasattr(l, 'num_features'): \n",
    "            return l.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_db = save_fc1(data, body, learn.path, 'resnet50', TMP_PATH, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([64, 4096]) torch.Size([64, 300])\n",
      "torch.Size([28, 4096]) torch.Size([28, 300])\n"
     ]
    }
   ],
   "source": [
    "for x,y in iter(fc_db.valid_dl):\n",
    "        print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train our custom head from our computed activations. Notice that we are training our head **as part of the original custom ConvNet** and not as a separate sequential object. This is important because it means that we can load our pretrained model into our backbone like we did before, train our head only with the precomputed activations and then train the whole network **without having to join our two parts** (they are trained separately but are still connected in our memory). In summary, once we have loaded our pretrained weights on our backbone and trained our head we can directly train our whole network with differential learning rates without having to do any adjustment. Cool huh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = learn.model[1][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Dropout(p=0.2)\n",
       "  (4): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (5): ReLU(inplace)\n",
       "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Dropout(p=0.2)\n",
       "  (8): Linear(in_features=512, out_features=300, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_head = Learner(data=fc_db, model=head, opt_fn = partial(AdamW, betas=(0.9,0.99)), loss_fn = cos_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685a9c8e8a0647cdabd9584b3cde25c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value='0.00% [0/1 00:00<00:00]'))), HTML(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_head.lr_find(start_lr=1e-4, end_lr=1e15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_head.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "wd = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7587b6b6f6e44c618b4ed1c18139c421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=10), HTML(value='0.00% [0/10 00:00<00:00]'))), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-660ccc6ef7c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, wd, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     cbs = [OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n\u001b[1;32m     17\u001b[0m                              pct_start=pct_start, **kwargs)]\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         fit(epochs, self.model, self.loss_fn, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0;32m--> 133\u001b[0;31m             callbacks=self.callbacks+callbacks)\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_fn, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_fn, opt, cb_handler, metrics)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_head.fit_one_cycle(10, lr, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_head.save('pre0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.array([lr/1000,lr/100,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, lrs, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search imagenet classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns, wvs = list(zip(*syn_wv_1k))\n",
    "wvs = np.array(wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "%time pred_wv = learn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "denorm = md.val_ds.denorm\n",
    "\n",
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "def show_imgs(ims, cols, figsize=None):\n",
    "    fig,axes = plt.subplots(len(ims)//cols, cols, figsize=figsize)\n",
    "    for i,ax in enumerate(axes.flat): show_img(ims[i], ax=ax)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e5f8ae797ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_imgs' is not defined"
     ]
    }
   ],
   "source": [
    "show_imgs(denorm(md.val_ds[start:start+25][0]), 5, (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "\n",
    "def create_index(a):\n",
    "    index = nmslib.init(space='angulardist')\n",
    "    index.addDataPointBatch(a)\n",
    "    index.createIndex()\n",
    "    return index\n",
    "\n",
    "def get_knns(index, vecs):\n",
    "     return zip(*index.knnQueryBatch(vecs, k=10, num_threads=4))\n",
    "\n",
    "def get_knn(index, vec): return index.knnQuery(vec, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_wvs = create_index(wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs,dists = get_knns(nn_wvs, pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['spoonbill', 'bustard', 'oystercatcher'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[classids[syns[id]] for id in ids[:3]] for ids in idxs[start:start+10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search all wordnet noun classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_syns, all_wvs = list(zip(*syn2wv.items()))\n",
    "all_wvs = np.array(all_wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_allwvs = create_index(all_wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs,dists = get_knns(nn_allwvs, pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['spoonbill', 'bustard', 'oystercatcher'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[classids[all_syns[id]] for id in ids[:3]] for ids in idxs[start:start+10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text -> image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_predwv = create_index(pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecd = pickle.load(open(TRANS_PATH/'wiki.en.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = en_vecd['boat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = (en_vecd['engine'] + en_vecd['boat'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = (en_vecd['sail'] + en_vecd['boat'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image->image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'valid/n01440764/ILSVRC2012_val_00007197.JPEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open_image(PATH/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c9b680a767fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_img' is not defined"
     ]
    }
   ],
   "source": [
    "show_img(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_img = md.val_ds.transform(img)\n",
    "pred = learn.predict_array(t_img[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cf37ec4574c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, pred)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[1:4]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
